{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "def eigen_variance(net, criterion, dataloader, n_iters=10, tol=1e-2, verbose=False):\n",
    "    n_parameters = num_parameters(net)\n",
    "    v0 = torch.randn(n_parameters)\n",
    "\n",
    "    Av_func = lambda v: variance_vec_prod(net, criterion, dataloader, v)\n",
    "    mu = power_method(v0, Av_func, n_iters, tol, verbose)\n",
    "    return mu\n",
    "\n",
    "\n",
    "def eigen_hessian(net, criterion, dataloader, n_iters=10, tol=1e-2, verbose=False):\n",
    "    n_parameters = num_parameters(net)\n",
    "    v0 = torch.randn(n_parameters)\n",
    "\n",
    "    Av_func = lambda v: hessian_vec_prod(net, criterion, dataloader, v)\n",
    "    mu = power_method(v0, Av_func, n_iters, tol, verbose)\n",
    "    return mu\n",
    "\n",
    "\n",
    "def variance_vec_prod(net, criterion, dataloader, v):\n",
    "    X, y = dataloader.X, dataloader.y\n",
    "    Av, Hv, n_samples = 0, 0, len(y)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        bx, by = X[i:i+1].cuda(), y[i:i+1].cuda()\n",
    "        Hv_i = Hv_batch(net, criterion, bx, by, v)\n",
    "        Av_i = Hv_batch(net, criterion, bx, by, Hv_i)\n",
    "        Av += Av_i\n",
    "        Hv += Hv_i\n",
    "    Av /= n_samples\n",
    "    Hv /= n_samples\n",
    "    H2v = hessian_vec_prod(net, criterion, dataloader, Hv)\n",
    "    return Av - H2v\n",
    "\n",
    "\n",
    "def hessian_vec_prod(net, criterion, dataloader, v):\n",
    "    Hv_t = 0\n",
    "    n_batchs = len(dataloader)\n",
    "    dataloader.idx = 0\n",
    "    for _ in range(n_batchs):\n",
    "        bx, by = next(dataloader)\n",
    "        Hv_t += Hv_batch(net, criterion, bx.cuda(), by.cuda(), v)\n",
    "\n",
    "    return Hv_t/n_batchs\n",
    "\n",
    "\n",
    "def Hv_batch(net, criterion, batch_x, batch_y, v):\n",
    "    \"\"\"\n",
    "    Hessian vector multiplication\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    logits = net(batch_x)\n",
    "    loss = criterion(logits, batch_y)\n",
    "\n",
    "    grads = autograd.grad(loss, net.parameters(), create_graph=True, retain_graph=True)\n",
    "    idx, res = 0, 0\n",
    "    for grad_i in grads:\n",
    "        ng = torch.numel(grad_i)\n",
    "        v_i = v[idx:idx+ng].cuda()\n",
    "        res += torch.dot(v_i, grad_i.view(-1))\n",
    "        idx += ng\n",
    "\n",
    "    Hv = autograd.grad(res, net.parameters())\n",
    "    Hv = [t.data.cpu().view(-1) for t in Hv]\n",
    "    Hv = torch.cat(Hv)\n",
    "    return Hv\n",
    "\n",
    "\n",
    "def power_method(v0, Av_func, n_iters=10, tol=1e-3, verbose=False):\n",
    "    mu = 0\n",
    "    v = v0/v0.norm()\n",
    "    for i in range(n_iters):\n",
    "        time_start = time.time()\n",
    "\n",
    "        Av = Av_func(v)\n",
    "        mu_pre = mu\n",
    "        mu = torch.dot(Av,v).item()\n",
    "        v = Av/Av.norm()\n",
    "\n",
    "        if abs(mu-mu_pre)/abs(mu) < tol:\n",
    "            break\n",
    "        if verbose:\n",
    "            print('%d-th step takes %.0f seconds, \\t %.2e'%(i+1,time.time()-time_start,mu))\n",
    "    return mu\n",
    "\n",
    "\n",
    "def num_parameters(net):\n",
    "    \"\"\"\n",
    "    return the number of parameters for given model\n",
    "    \"\"\"\n",
    "    n_parameters = 0\n",
    "    for para in net.parameters():\n",
    "        n_parameters += para.data.numel()\n",
    "\n",
    "    return n_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "0 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "1 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "2 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "3 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "4 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "5 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "6 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "7 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "8 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "9 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "10 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "11 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "12 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "13 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "14 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "15 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "16 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "17 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "18 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "19 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "20 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "21 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "22 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "23 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "24 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "25 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "26 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "27 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "28 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "29 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "0 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "1 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "2 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n",
      "3 torch.Size([500, 3, 32, 32]) torch.Size([500, 2])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "\n",
    "    def __init__(self,X,y,batch_size):\n",
    "        self.X, self.y = X, y \n",
    "        self.batch_size = batch_size\n",
    "        self.n_samples = len(y)\n",
    "        self.idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        length = self.n_samples // self.batch_size\n",
    "        if self.n_samples > length * self.batch_size:\n",
    "            length += 1\n",
    "        return length\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self    \n",
    "\n",
    "    def __next__(self):\n",
    "        if self.idx >= self.n_samples:\n",
    "            self.idx = 0\n",
    "            rnd_idx = torch.randperm(self.n_samples)\n",
    "            self.X = self.X[rnd_idx]\n",
    "            self.y = self.y[rnd_idx]\n",
    "\n",
    "        idx_end = min(self.idx+self.batch_size, self.n_samples)\n",
    "        batch_X = self.X[self.idx:idx_end]\n",
    "        batch_y = self.y[self.idx:idx_end]\n",
    "        self.idx = idx_end\n",
    "\n",
    "        return batch_X,batch_y\n",
    "\n",
    "\n",
    "def load_fmnist(training_size, batch_size=100):\n",
    "    train_set = dsets.FashionMNIST('data/fashionmnist', train=True, download=True)\n",
    "    train_X, train_y = train_set.data[0:training_size].float()/255, \\\n",
    "                       to_one_hot(train_set.targets[0:training_size])\n",
    "    train_loader = DataLoader(train_X, train_y, batch_size)\n",
    "\n",
    "    test_set = dsets.FashionMNIST('data/fashionmnist', train=False,download=True)\n",
    "    test_X, test_y = test_set.data.float()/255, \\\n",
    "                     to_one_hot(test_set.targets)\n",
    "    test_loader = DataLoader(test_X, test_y, batch_size)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def load_cifar10(training_size, batch_size=100):\n",
    "    \"\"\"\n",
    "    load cifar10 dataset. Notice that here we only use examples\n",
    "    corresponding to label 0 and 1. Thus the training_size is at \n",
    "    most 10000.\n",
    "    \"\"\"\n",
    "    train_set = dsets.CIFAR10('data/cifar10', train=True, download=True)\n",
    "    train_X,train_y = modify_cifar_data(train_set.data, train_set.targets, training_size)\n",
    "    train_loader = DataLoader(train_X, train_y, batch_size)\n",
    "\n",
    "    test_set = dsets.CIFAR10('data/cifar10', train=False, download=True)\n",
    "    test_X,test_y = modify_cifar_data(test_set.data, test_set.targets)\n",
    "    test_loader = DataLoader(test_X, test_y, batch_size)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def modify_cifar_data(X, y, n_samples=-1):\n",
    "    X = torch.from_numpy(X.transpose([0,3,1,2]))\n",
    "    y = torch.LongTensor(y)\n",
    "\n",
    "    X_t = torch.Tensor(50000,3,32,32)\n",
    "    y_t = torch.LongTensor(50000)\n",
    "    idx = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0 or y[i] == 1:\n",
    "            y_t[idx] = y[i]\n",
    "            X_t[idx,:,:,:] = X[i,:,:,:]\n",
    "            idx += 1\n",
    "    X = X_t[0:idx]\n",
    "    y = y_t[0:idx] \n",
    "\n",
    "    if n_samples > 1:\n",
    "        X = X[0:n_samples]\n",
    "        y = y[0:n_samples]\n",
    "\n",
    "    # preprocess the data\n",
    "    X = X.float()/255.0\n",
    "    y = to_one_hot(y) \n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def to_one_hot(labels):\n",
    "    if labels.ndimension()==1:\n",
    "        labels.unsqueeze_(1)\n",
    "    n_samples = labels.shape[0]\n",
    "    n_classes = labels.max()+1\n",
    "\n",
    "    one_hot_labels = torch.FloatTensor(n_samples,n_classes)\n",
    "    one_hot_labels.zero_()\n",
    "    one_hot_labels.scatter_(1, labels, 1)\n",
    "\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_loader, test_loader = load_cifar10(training_size=10000,batch_size=500)\n",
    "    for i in range(30):\n",
    "        batch_x, batch_y = next(train_loader)\n",
    "        print(i, batch_x.shape, batch_y.shape)\n",
    "\n",
    "    for i in range(4):\n",
    "        batch_x, batch_y = next(test_loader)\n",
    "        print(i, batch_x.shape, batch_y.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def train(model, criterion, optimizer, dataloader, batch_size, n_iters=50000, verbose=True):\n",
    "    model.train()\n",
    "    acc_avg, loss_avg = 0, 0\n",
    "\n",
    "    since = time.time()\n",
    "    for iter_now in range(n_iters):\n",
    "        optimizer.zero_grad()\n",
    "        loss,acc = compute_minibatch_gradient(model, criterion, dataloader, batch_size)\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_avg = 0.9 * acc_avg + 0.1 * acc if acc_avg > 0 else acc\n",
    "        loss_avg = 0.9 * loss_avg + 0.1 * loss if loss_avg > 0 else loss\n",
    "\n",
    "        if iter_now%200 == 0 and verbose:\n",
    "            now = time.time()\n",
    "            print('%d/%d, took %.0f seconds, train_loss: %.1e, train_acc: %.2f'%(\n",
    "                    iter_now+1, n_iters, now-since, loss_avg, acc_avg))\n",
    "            since = time.time()\n",
    "\n",
    "\n",
    "def compute_minibatch_gradient(model, criterion, dataloader, batch_size):\n",
    "    loss,acc = 0,0\n",
    "    n_loads = batch_size // dataloader.batch_size\n",
    "\n",
    "    for i in range(n_loads):\n",
    "        inputs,targets = next(dataloader)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        logits = model(inputs)\n",
    "        E = criterion(logits,targets)\n",
    "        E.backward()\n",
    "        \n",
    "        loss += E.item()\n",
    "        acc += accuracy(logits.data,targets)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad.data /= n_loads\n",
    "\n",
    "    return loss/n_loads, acc/n_loads\n",
    "\n",
    "\n",
    "def accuracy(logits, targets):\n",
    "    n = logits.shape[0]\n",
    "    if targets.ndimension() == 2:\n",
    "        _, y_trues = torch.max(targets,1)\n",
    "    else:\n",
    "        y_trues = targets \n",
    "    _, y_preds = torch.max(logits,1)\n",
    "\n",
    "    acc = (y_trues==y_preds).float().sum()*100.0/n \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Modified from https://github.com/pytorch/vision.git\n",
    "'''\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "__all__ = [\n",
    "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
    "    'vgg19_bn', 'vgg19',\n",
    "]\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model\n",
    "    '''\n",
    "    def __init__(self, features,feature_size=512,num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_size, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128,num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'A': [16, 'M', 16, 'M', 32, 'M',  64, 'M', 64, 'M'],\n",
    "    'A1': [16, 'M', 32, 'M', 32, 32, 'M', 64, 64, 'M', 128, 128, 'M'],\n",
    "    'A2': [32, 'M', 64, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M'],\n",
    "    'A3': [64, 'M', 128, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'A4': [128, 'M', 256, 'M', 256, 256, 'M', 512, 512, 'M', 1024, 1024, 'M'],\n",
    "    'B': [16, 16, 'M', 32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 128, 128, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',\n",
    "          512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "def vgg11(num_classes=10):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
    "    return VGG(make_layers(cfg['A']),feature_size=64,num_classes=num_classes)\n",
    "\n",
    "\n",
    "def vgg11_big(num_classes=10):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
    "    return VGG(make_layers(cfg['A3']),cfg['A3'][-2],num_classes)\n",
    "\n",
    "def vgg11_bn(num_classes):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['A'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg13(num_classes=10):\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
    "    return VGG(make_layers(cfg['B']),num_classes)\n",
    "\n",
    "\n",
    "def vgg13_bn():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg16():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
    "    return VGG(make_layers(cfg['D']))\n",
    "\n",
    "\n",
    "def vgg16_bn():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
    "    return VGG(make_layers(cfg['E']))\n",
    "\n",
    "\n",
    "def vgg19_bn():\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['E'], batch_norm=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5,stride=1) # 28-5+1=24\n",
    "        self.conv2 = nn.Conv2d(6,16,5,stride=1) # 12-5+1=8\n",
    "        self.fc1 = nn.Linear(4*4*16,200)\n",
    "        self.fc2 = nn.Linear(200,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        if x.ndimension()==3:\n",
    "            x = x.unsqueeze(0)\n",
    "        o = F.relu(self.conv1(x))\n",
    "        o = F.avg_pool2d(o,2,2)\n",
    "\n",
    "        o = F.relu(self.conv2(o))\n",
    "        o = F.avg_pool2d(o,2,2)\n",
    "\n",
    "        o = o.view(o.shape[0],-1)\n",
    "        o = self.fc1(o)\n",
    "        o = F.relu(o)\n",
    "        o = self.fc2(o)\n",
    "        return o\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FNN,self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(784,500),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(500,500),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(500,500),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(500,10))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        o = self.net(x)\n",
    "        return o\n",
    "\n",
    "\n",
    "def lenet():\n",
    "    return LeNet()\n",
    "\n",
    "def fnn():\n",
    "    return FNN()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# from .models.vgg import vgg11\n",
    "# from .models.mnist import fnn\n",
    "# from .data import load_fmnist,load_cifar10\n",
    "# from .trainer import accuracy\n",
    "# from .linalg import eigen_variance, eigen_hessian\n",
    "\n",
    "\n",
    "\n",
    "# def load_net(dataset):\n",
    "#     if dataset == 'fashionmnist':\n",
    "#             return fnn().cuda()\n",
    "#     elif dataset == 'cifar10':\n",
    "#             return vgg11(num_classes=2).cuda()\n",
    "#     else:\n",
    "#         raise ValueError('Dataset %s is not supported'%(dataset))\n",
    "\n",
    "\n",
    "# def load_data(dataset, training_size, batch_size):\n",
    "#     if dataset == 'fashionmnist':\n",
    "#             return load_fmnist(training_size, batch_size)\n",
    "#     elif dataset == 'cifar10':\n",
    "#             return load_cifar10(training_size, batch_size)\n",
    "#     else:\n",
    "#         raise ValueError('Dataset %s is not supported'%(dataset))\n",
    "\n",
    "\n",
    "# def get_sharpness(net, criterion, dataloader, n_iters=10, tol=1e-2, verbose=False):\n",
    "#     v = eigen_hessian(net, criterion, dataloader, \\\n",
    "#                       n_iters=n_iters, tol=tol, verbose=verbose)\n",
    "#     return v\n",
    "\n",
    "\n",
    "# def get_nonuniformity(net, criterion, dataloader, n_iters=10, tol=1e-2, verbose=False):\n",
    "#     v = eigen_variance(net, criterion, dataloader, \\\n",
    "#                       n_iters=n_iters, tol=tol, verbose=verbose)\n",
    "#     return math.sqrt(v)\n",
    "\n",
    "\n",
    "def eval_accuracy(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    n_batchs = len(dataloader)\n",
    "    dataloader.idx = 0\n",
    "\n",
    "    loss_t, acc_t = 0.0, 0.0\n",
    "    for i in range(n_batchs):\n",
    "        inputs,targets = next(dataloader)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        logits = model(inputs)\n",
    "        loss_t += criterion(logits,targets).item()\n",
    "        acc_t += accuracy(logits.data,targets)\n",
    "\n",
    "    return loss_t/n_batchs, acc_t/n_batchs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.1\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "===> Architecture:\n",
      "FNN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=500, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "===> Start training\n",
      "1/10000, took 0 seconds, train_loss: 9.9e-02, train_acc: 4.70\n",
      "201/10000, took 2 seconds, train_loss: 6.7e-02, train_acc: 59.92\n",
      "401/10000, took 2 seconds, train_loss: 5.2e-02, train_acc: 69.31\n",
      "601/10000, took 2 seconds, train_loss: 4.3e-02, train_acc: 73.48\n",
      "801/10000, took 2 seconds, train_loss: 3.9e-02, train_acc: 76.45\n",
      "1001/10000, took 2 seconds, train_loss: 3.5e-02, train_acc: 79.87\n",
      "1201/10000, took 2 seconds, train_loss: 3.3e-02, train_acc: 81.22\n",
      "1401/10000, took 2 seconds, train_loss: 3.0e-02, train_acc: 83.86\n",
      "1601/10000, took 2 seconds, train_loss: 2.8e-02, train_acc: 85.79\n",
      "1801/10000, took 2 seconds, train_loss: 2.6e-02, train_acc: 87.17\n",
      "2001/10000, took 2 seconds, train_loss: 2.5e-02, train_acc: 88.52\n",
      "2201/10000, took 2 seconds, train_loss: 2.3e-02, train_acc: 90.03\n",
      "2401/10000, took 2 seconds, train_loss: 2.2e-02, train_acc: 90.72\n",
      "2601/10000, took 2 seconds, train_loss: 2.0e-02, train_acc: 91.79\n",
      "2801/10000, took 2 seconds, train_loss: 1.9e-02, train_acc: 92.57\n",
      "3001/10000, took 2 seconds, train_loss: 1.8e-02, train_acc: 92.99\n",
      "3201/10000, took 2 seconds, train_loss: 1.7e-02, train_acc: 93.78\n",
      "3401/10000, took 2 seconds, train_loss: 1.6e-02, train_acc: 94.71\n",
      "3601/10000, took 2 seconds, train_loss: 1.5e-02, train_acc: 95.31\n",
      "3801/10000, took 2 seconds, train_loss: 1.5e-02, train_acc: 95.78\n",
      "4001/10000, took 2 seconds, train_loss: 1.4e-02, train_acc: 96.27\n",
      "4201/10000, took 2 seconds, train_loss: 1.3e-02, train_acc: 96.79\n",
      "4401/10000, took 2 seconds, train_loss: 1.3e-02, train_acc: 97.17\n",
      "4601/10000, took 2 seconds, train_loss: 1.2e-02, train_acc: 97.27\n",
      "4801/10000, took 2 seconds, train_loss: 1.1e-02, train_acc: 97.79\n",
      "5001/10000, took 2 seconds, train_loss: 1.1e-02, train_acc: 97.99\n",
      "5201/10000, took 2 seconds, train_loss: 1.0e-02, train_acc: 98.32\n",
      "5401/10000, took 2 seconds, train_loss: 9.7e-03, train_acc: 98.51\n",
      "5601/10000, took 2 seconds, train_loss: 9.2e-03, train_acc: 98.63\n",
      "5801/10000, took 2 seconds, train_loss: 8.5e-03, train_acc: 98.89\n",
      "6001/10000, took 2 seconds, train_loss: 8.5e-03, train_acc: 98.74\n",
      "6201/10000, took 2 seconds, train_loss: 7.6e-03, train_acc: 99.13\n",
      "6401/10000, took 2 seconds, train_loss: 7.4e-03, train_acc: 99.29\n",
      "6601/10000, took 2 seconds, train_loss: 7.2e-03, train_acc: 99.29\n",
      "6801/10000, took 2 seconds, train_loss: 6.7e-03, train_acc: 99.50\n",
      "7001/10000, took 2 seconds, train_loss: 6.0e-03, train_acc: 99.63\n",
      "7201/10000, took 2 seconds, train_loss: 5.9e-03, train_acc: 99.68\n",
      "7401/10000, took 2 seconds, train_loss: 5.5e-03, train_acc: 99.80\n",
      "7601/10000, took 2 seconds, train_loss: 5.5e-03, train_acc: 99.86\n",
      "7801/10000, took 2 seconds, train_loss: 5.0e-03, train_acc: 99.85\n",
      "8001/10000, took 2 seconds, train_loss: 5.0e-03, train_acc: 99.94\n",
      "8201/10000, took 2 seconds, train_loss: 4.5e-03, train_acc: 99.90\n",
      "8401/10000, took 2 seconds, train_loss: 4.3e-03, train_acc: 99.90\n",
      "8601/10000, took 2 seconds, train_loss: 4.2e-03, train_acc: 99.95\n",
      "8801/10000, took 2 seconds, train_loss: 3.7e-03, train_acc: 99.94\n",
      "9001/10000, took 2 seconds, train_loss: 3.6e-03, train_acc: 99.95\n",
      "9201/10000, took 2 seconds, train_loss: 3.4e-03, train_acc: 100.00\n",
      "9401/10000, took 2 seconds, train_loss: 3.1e-03, train_acc: 99.98\n",
      "9601/10000, took 2 seconds, train_loss: 3.1e-03, train_acc: 100.00\n",
      "9801/10000, took 2 seconds, train_loss: 2.9e-03, train_acc: 99.99\n",
      "===> Solution: \n",
      "\t train loss: 2.87e-03, acc: 100.00\n",
      "\t test loss: 3.36e-02, acc: 80.47\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# from src.trainer import train\n",
    "# from src.utils import load_net, load_data, eval_accuracy\n",
    "\n",
    "\n",
    "\n",
    "# def get_args():\n",
    "#     argparser = argparse.ArgumentParser(description=__doc__)\n",
    "#     argparser.add_argument('--gpuid',\n",
    "#                           default='0,', help='gpu id, [0] ')\n",
    "#     argparser.add_argument('--dataset', \n",
    "#                           default='fashionmnist', help='dataset, [fashionmnist] | cifar10')\n",
    "#     argparser.add_argument('--n_samples', type=int,\n",
    "#                            default=1000, help='training set size, [1000]')\n",
    "#     argparser.add_argument('--load_size', type=int,\n",
    "#                            default=1000, help='load size for dataset, [1000]')\n",
    "#     argparser.add_argument('--optimizer', \n",
    "#                            default='sgd', help='optimizer, [sgd]')\n",
    "#     argparser.add_argument('--n_iters', type=int,\n",
    "#                            default=10000, help='number of iteration used to train nets, [10000]')\n",
    "#     argparser.add_argument('--batch_size', type=int,\n",
    "#                            default=1000, help='batch size, [1000]')\n",
    "#     argparser.add_argument('--learning_rate', type=float,\n",
    "#                            default=1e-1, help='learning rate')\n",
    "#     argparser.add_argument('--momentum', type=float,\n",
    "#                            default='0.0', help='momentum, [0.0]')\n",
    "#     argparser.add_argument('--model_file', \n",
    "#                            default='fnn.pkl', help='filename to save the net, fnn.pkl')\n",
    "\n",
    "#     args = argparser.parse_args()\n",
    "#     if args.load_size > args.batch_size:\n",
    "#         raise ValueError('load size should not be larger than batch size')\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpuid\n",
    "\n",
    "#     print('===> Config:')\n",
    "#     print(json.dumps(vars(args), indent=2))\n",
    "#     return args\n",
    "\n",
    "# def get_optimizer(net, optimizer):\n",
    "#     if optimizer == 'sgd':\n",
    "#         return torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "#     elif optimizer == 'adam':\n",
    "#         return torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#     else:\n",
    "#         raise ValueError('optimizer %s has not been supported'%(optimizer))\n",
    "\n",
    "# args = get_args()\n",
    "\n",
    "criterion = torch.nn.MSELoss().cuda()\n",
    "\n",
    "\n",
    "# train_loader, test_loader = load_data(args.dataset,\n",
    "#                                       training_size=args.n_samples,\n",
    "#                                       batch_size=args.load_size)\n",
    "# net = load_net(args.dataset)\n",
    "# optimizer = get_optimizer(net, args)\n",
    "# print(optimizer)\n",
    "\n",
    "# print('===> Architecture:')\n",
    "# print(net)\n",
    "\n",
    "# print('===> Start training')\n",
    "# train(net, criterion, optimizer, train_loader, args.batch_size, args.n_iters, verbose=True)\n",
    "\n",
    "# train_loss, train_accuracy = eval_accuracy(net, criterion, train_loader)\n",
    "# test_loss, test_accuracy = eval_accuracy(net, criterion, test_loader)\n",
    "# print('===> Solution: ')\n",
    "# print('\\t train loss: %.2e, acc: %.2f' % (train_loss, train_accuracy))\n",
    "# print('\\t test loss: %.2e, acc: %.2f' % (test_loss, test_accuracy))\n",
    "\n",
    "# torch.save(net.state_dict(), args.model_file)\n",
    "\n",
    "\n",
    "gpuid = '0,'\n",
    "dataset= 'fashionmnist'\n",
    "n_samples = 1000\n",
    "load_size = 1000\n",
    "optimizer = 'sgd'\n",
    "n_iters = 10000\n",
    "batch_size = 1000\n",
    "learning_rate = 1e-1\n",
    "momentum = 0.0\n",
    "model_file = 'fnn.pkl'\n",
    "\n",
    "\n",
    "# criterion = torch.nn.MSELoss().cuda()\n",
    "# train_loader, test_loader = load_data(dataset,\n",
    "#                                       training_size=n_samples,\n",
    "#                                       batch_size=load_size)\n",
    "\n",
    "# net = load_net(dataset)\n",
    "\n",
    "if dataset == 'fashionmnist':\n",
    "        train_loader, test_loader = load_fmnist(training_size=n_samples, batch_size=load_size)\n",
    "        net = fnn().cuda()\n",
    "        \n",
    "elif dataset == 'cifar10':\n",
    "        train_loader, test_loader = load_cifar10(training_size=n_samples, batch_size=load_size)\n",
    "        net = vgg11(num_classes=2).cuda()\n",
    "\n",
    "# optimizer = get_optimizer(net, optimizer)\n",
    "\n",
    "if optimizer == 'sgd':\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "elif optimizer == 'adam':\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "print(optimizer)\n",
    "\n",
    "print('===> Architecture:')\n",
    "print(net)\n",
    "\n",
    "print('===> Start training')\n",
    "train(net, criterion, optimizer, train_loader, batch_size, n_iters, verbose=True)\n",
    "\n",
    "train_loss, train_accuracy = eval_accuracy(net, criterion, train_loader)\n",
    "test_loss, test_accuracy = eval_accuracy(net, criterion, test_loader)\n",
    "print('===> Solution: ')\n",
    "print('\\t train loss: %.2e, acc: %.2f' % (train_loss, train_accuracy))\n",
    "print('\\t test loss: %.2e, acc: %.2f' % (test_loss, test_accuracy))\n",
    "\n",
    "# torch.save(net.state_dict(), model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Config:\n",
      "===> Basic information of the given model: \n",
      "\t train loss: 2.87e-03, acc: 100.00\n",
      "\t test loss: 3.36e-02, acc: 80.47\n",
      "===> Compute sharpness:\n",
      "Sharpness is 1.98e+01\n",
      "\n",
      "===> Compute non-uniformity:\n",
      "1-th step takes 35 seconds, \t 9.02e-03\n",
      "2-th step takes 36 seconds, \t 9.82e+02\n",
      "3-th step takes 36 seconds, \t 1.31e+03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-80d9407e59fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;31m#                                     n_iters=10, verbose=True, tol=1e-4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m \u001b[0mnon_uniformity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meigen_variance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Non-uniformity is %.2e\\n'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnon_uniformity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-2aa2ddb14b3d>\u001b[0m in \u001b[0;36meigen_variance\u001b[1;34m(net, criterion, dataloader, n_iters, tol, verbose)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mAv_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvariance_vec_prod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpower_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAv_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-2aa2ddb14b3d>\u001b[0m in \u001b[0;36mpower_method\u001b[1;34m(v0, Av_func, n_iters, tol, verbose)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mtime_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mAv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAv_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mmu_pre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-2aa2ddb14b3d>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mv0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mAv_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvariance_vec_prod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpower_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAv_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-2aa2ddb14b3d>\u001b[0m in \u001b[0;36mvariance_vec_prod\u001b[1;34m(net, criterion, dataloader, v)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mbx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mHv_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHv_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mAv_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHv_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHv_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mAv\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mAv_i\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-2aa2ddb14b3d>\u001b[0m in \u001b[0;36mHv_batch\u001b[1;34m(net, criterion, batch_x, batch_y, v)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mgrad_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    155\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[0;32m    156\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         inputs, allow_unused)\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# from src.utils import load_net, load_data, \\\n",
    "#                       get_sharpness, get_nonuniformity, \\\n",
    "#                       eval_accuracy\n",
    "\n",
    "\n",
    "\n",
    "gpuid = '0,'\n",
    "dataset= 'fashionmnist'\n",
    "n_samples = 1000\n",
    "batch_size = 1000\n",
    "model_file = 'fnn.pkl'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpuid\n",
    "print('===> Config:')\n",
    "\n",
    "# def get_args():\n",
    "#     argparser = argparse.ArgumentParser(description=__doc__)\n",
    "#     argparser.add_argument('--gpuid',default='0,')\n",
    "#     argparser.add_argument('--dataset',default='fashionmnist',\n",
    "#                             help='dataset choosed, [fashionmnist] | cifar10')\n",
    "#     argparser.add_argument('--n_samples',type=int,\n",
    "#                             default=1000, help='training set size, [1000]')\n",
    "#     argparser.add_argument('--batch_size', type=int,\n",
    "#                             default=1000, help='batch size')\n",
    "#     argparser.add_argument('--model_file', default='fnn.pkl',\n",
    "#                             help='file name of the pretrained model')\n",
    "#     args = argparser.parse_args()\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpuid\n",
    "\n",
    "#     print('===> Config:')\n",
    "#     print(json.dumps(vars(args),indent=2))\n",
    "#     return args\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load model\n",
    "# criterion = torch.nn.MSELoss().cuda()\n",
    "# train_loader,test_loader = load_data(dataset, \n",
    "#                                     training_size=n_samples, \n",
    "#                                     batch_size=batch_size)\n",
    "# net = load_net(dataset)\n",
    "# net.load_state_dict(torch.load(model_file))\n",
    "\n",
    "# # Evaluate models\n",
    "# train_loss, train_accuracy = eval_accuracy(net, criterion, train_loader)\n",
    "# test_loss, test_accuracy = eval_accuracy(net, criterion, test_loader)\n",
    "\n",
    "print('===> Basic information of the given model: ')\n",
    "print('\\t train loss: %.2e, acc: %.2f'%(train_loss, train_accuracy))\n",
    "print('\\t test loss: %.2e, acc: %.2f'%(test_loss, test_accuracy))\n",
    "\n",
    "print('===> Compute sharpness:')\n",
    "# sharpness = get_sharpness(net, criterion, train_loader, \\\n",
    "#                             n_iters=10, verbose=True, tol=1e-4)\n",
    "\n",
    "sharpness = eigen_hessian(net, criterion, train_loader, n_iters=10, tol=1e-4, verbose=False)\n",
    "\n",
    "print('Sharpness is %.2e\\n'%(sharpness))\n",
    "\n",
    "print('===> Compute non-uniformity:')\n",
    "# non_uniformity = get_nonuniformity(net, criterion, train_loader, \\\n",
    "#                                     n_iters=10, verbose=True, tol=1e-4)\n",
    "\n",
    "non_uniformity = math.sqrt(eigen_variance(net, criterion, train_loader, n_iters=10, tol=1e-4, verbose=True))\n",
    "\n",
    "print('Non-uniformity is %.2e\\n'%(non_uniformity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
