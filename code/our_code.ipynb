{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import time\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dsets\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader class represents a python iterable over the dataset. It helps us to read the dataset in chunks with dynamic batch size. It also shuffles the data for each epochs i.e. when the whole data read once. Input arguements are X, y and batch_size. It will return iterable batch_x, batch_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "\n",
    "    def __init__(self,X,y,batch_size):\n",
    "        self.X, self.y = X, y \n",
    "        self.batch_size = batch_size\n",
    "        self.n_samples = len(y)\n",
    "        self.idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(self.n_samples/self.batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self    \n",
    "\n",
    "    def __next__(self):\n",
    "        if self.idx >= self.n_samples:\n",
    "            self.idx = 0\n",
    "            rnd_idx = torch.randperm(self.n_samples)\n",
    "            self.X = self.X[rnd_idx]\n",
    "            self.y = self.y[rnd_idx]\n",
    "\n",
    "        idx_end = min(self.idx+self.batch_size, self.n_samples)\n",
    "        batch_X = self.X[self.idx:idx_end]\n",
    "        batch_y = self.y[self.idx:idx_end]\n",
    "        self.idx = idx_end\n",
    "\n",
    "        return batch_X,batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing one hot encoding on target label which basically convert categorical variables into binary vectors. Basically, each integer value in labels is converted into binary vector having all zero value except 1 at the index of the integer. Input size is [n_samples x 1] and output isze is [n_samples x n_classes]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels):\n",
    "    if labels.ndimension()==1:\n",
    "        labels.unsqueeze_(1)\n",
    "    n_samples = labels.shape[0]\n",
    "    n_classes = labels.max()+1\n",
    "\n",
    "    one_hot_labels = torch.FloatTensor(n_samples,n_classes)\n",
    "    one_hot_labels.zero_()\n",
    "    one_hot_labels.scatter_(1, labels, 1)\n",
    "\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the FashionMNIST train and test dataset. Dataset have images with 28x28 in size and have 10 classes. Converting image pixel from 0-255 range to 0.0-1.0 range by diving it by 255. Converting train_y into one hot encoded labels. Output is train and test dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fmnist(training_size, batch_size=100):\n",
    "    train_set = dsets.FashionMNIST('data/fashionmnist', train=True, download=True)\n",
    "    train_X = train_set.data[0:training_size].float()/255\n",
    "    train_y = to_one_hot(train_set.targets[0:training_size])\n",
    "    train_loader = DataLoader(train_X, train_y, batch_size)\n",
    "\n",
    "    test_set = dsets.FashionMNIST('data/fashionmnist', train=False,download=True)\n",
    "    test_X  = test_set.data.float()/255\n",
    "    test_y =  to_one_hot(test_set.targets)\n",
    "    test_loader = DataLoader(test_X, test_y, batch_size)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the CIFAR10 train and test dataset. Dataset have images with 32x32 in size and have 10 classes but we have only considered two classes, \"airplane\" and \"automobile\". Converting image pixel from 0-255 range to 0.0-1.0 range by diving it by 255. Converting train_y into one hot encoded labels. Output is train and test dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(training_size, batch_size=100):\n",
    "    train_set = dsets.CIFAR10('data/cifar10', train=True, download=True)\n",
    "    train_X,train_y = modify_cifar_data(train_set.data, train_set.targets, training_size)\n",
    "    train_loader = DataLoader(train_X, train_y, batch_size)\n",
    "\n",
    "    test_set = dsets.CIFAR10('data/cifar10', train=False, download=True)\n",
    "    test_X,test_y = modify_cifar_data(test_set.data, test_set.targets)\n",
    "    test_loader = DataLoader(test_X, test_y, batch_size)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def modify_cifar_data(X, y, n_samples=-1):\n",
    "    X = torch.from_numpy(X.transpose([0,3,1,2]))\n",
    "    y = torch.LongTensor(y)\n",
    "\n",
    "    X_t = torch.Tensor(50000,3,32,32)\n",
    "    y_t = torch.LongTensor(50000)\n",
    "    idx = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0 or y[i] == 1:\n",
    "            y_t[idx] = y[i]\n",
    "            X_t[idx,:,:,:] = X[i,:,:,:]\n",
    "            idx += 1\n",
    "    X = X_t[0:idx]\n",
    "    y = y_t[0:idx] \n",
    "\n",
    "    if n_samples > 1:\n",
    "        X = X[0:n_samples]\n",
    "        y = y[0:n_samples]\n",
    "\n",
    "    # preprocess the data\n",
    "    X = X.float()/255.0\n",
    "    y = to_one_hot(y) \n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing an example of loading CIFAR 10 dataset and iterating train and test loader to get batch_x and batch_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_cifar10(training_size=10000,batch_size=500)\n",
    "print(\"Train loader\")\n",
    "for i in range(2):\n",
    "    batch_x, batch_y = next(train_loader)\n",
    "    print(i, batch_x.shape, batch_y.shape)\n",
    "print(\"Test loader\")\n",
    "for i in range(2):\n",
    "    batch_x, batch_y = next(test_loader)\n",
    "    print(i, batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_x.shape)\n",
    "print(\"One hot encoded Labels: \", batch_y[:5])\n",
    "print(\"Image data: \", batch_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training models using pytorch. Serveral parameters are passed into the function: model (sgd), criterion (MSE Loss), dataloader (loading the training data), batch size of 1000 and 50000 iterations. We set all gradients of the model to zero, go through the batch gradient descent once with the given batch size. The avg_accuracy is changed by 10% of the minibatch each iteraiton. Every 200 iterations, we print iterations, loss, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, dataloader, batch_size, n_iters=50000, verbose=True):\n",
    "    model.train()\n",
    "    acc_avg = 0  \n",
    "    loss_avg = 0 \n",
    "    since = time.time()\n",
    "    for current_iter in range(n_iters):\n",
    "        optimizer.zero_grad()\n",
    "        loss,acc = compute_minibatch_GD(model, criterion, dataloader, batch_size)\n",
    "        optimizer.step()\n",
    "        if acc_avg > 0:\n",
    "            acc_avg = 0.9 * acc_avg + 0.1 * acc\n",
    "        else:\n",
    "            acc_avg = acc\n",
    "        if loss_avg > 0:\n",
    "            loss_avg = 0.9 * loss_avg + 0.1 * loss\n",
    "        else: \n",
    "            loss_avg = loss\n",
    "        if current_iter%200 == 0:\n",
    "            now = time.time()\n",
    "            print('%d/%d, took %.0f seconds, train_loss: %.1e, train_acc: %.2f'%(\n",
    "                    current_iter+1, n_iters, now-since, loss_avg, acc_avg))\n",
    "            since = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the minibatch gradient while iterating through the data from the data loader in increments of the batch size. Targets are the deisired outputs and logits are the predictions. Accuracy and lass are averaged across the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_minibatch_GD(model, criterion, dataloader, batch_size):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    n_loads = batch_size // dataloader.batch_size\n",
    "\n",
    "    for i in range(n_loads):\n",
    "        inputs, targets = next(dataloader)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        logits = model(inputs)\n",
    "        E = criterion(logits,targets)\n",
    "        E.backward()\n",
    "        acc += accuracy(logits.data,targets)\n",
    "        loss += E.item()\n",
    "        \n",
    "    for p in model.parameters():\n",
    "        p.grad.data /= n_loads\n",
    "    loss=loss/n_loads\n",
    "    acc=acc/n_loads\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the number of rows to n, which is the total number of targets/predictions. We compare the targets to the predictions and get the accuracy from the matches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, targets):\n",
    "    n = logits.shape[0]\n",
    "    if targets.ndimension() == 2:\n",
    "        _, y_trues = torch.max(targets,1)\n",
    "    else:\n",
    "        y_trues = targets \n",
    "    _, y_preds = torch.max(logits,1)\n",
    "\n",
    "    acc = (y_trues==y_preds).float().sum()\n",
    "    acc=acc*100.0/n \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(model, criterion, dataloader):\n",
    "    model.eval()\n",
    "    n_batchs = len(dataloader)\n",
    "    dataloader.idx = 0\n",
    "\n",
    "    loss_t, acc_t = 0.0, 0.0\n",
    "    for i in range(n_batchs):\n",
    "        inputs,targets = next(dataloader)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        logits = model(inputs)\n",
    "        loss_t += criterion(logits,targets).item()\n",
    "        acc_t += accuracy(logits.data,targets)\n",
    "\n",
    "    return loss_t/n_batchs, acc_t/n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Modified from https://github.com/pytorch/vision.git\n",
    "'''\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
    "    'vgg19_bn', 'vgg19',\n",
    "]\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    '''\n",
    "    VGG model\n",
    "    '''\n",
    "    def __init__(self, features,feature_size=512,num_classes=10):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_size, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128,num_classes),\n",
    "        )\n",
    "         # Initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'A': [16, 'M', 16, 'M', 32, 'M',  64, 'M', 64, 'M'],\n",
    "    'A1': [16, 'M', 32, 'M', 32, 32, 'M', 64, 64, 'M', 128, 128, 'M'],\n",
    "    'A2': [32, 'M', 64, 'M', 64, 64, 'M', 128, 128, 'M', 256, 256, 'M'],\n",
    "    'A3': [64, 'M', 128, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'A4': [128, 'M', 256, 'M', 256, 256, 'M', 512, 512, 'M', 1024, 1024, 'M'],\n",
    "    'B': [16, 16, 'M', 32, 32, 'M', 64, 64, 'M', 128, 128, 'M', 128, 128, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',\n",
    "          512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "def vgg11(num_classes=10):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
    "    return VGG(make_layers(cfg['A']),feature_size=64,num_classes=num_classes)\n",
    "\n",
    "\n",
    "def vgg11_big(num_classes=10):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
    "    return VGG(make_layers(cfg['A3']),cfg['A3'][-2],num_classes)\n",
    "\n",
    "def vgg11_bn(num_classes):\n",
    "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['A'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg13(num_classes=10):\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
    "    return VGG(make_layers(cfg['B']),num_classes)\n",
    "\n",
    "\n",
    "def vgg13_bn():\n",
    "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg16():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
    "    return VGG(make_layers(cfg['D']))\n",
    "\n",
    "\n",
    "def vgg16_bn():\n",
    "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
    "\n",
    "\n",
    "def vgg19():\n",
    "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
    "    return VGG(make_layers(cfg['E']))\n",
    "\n",
    "\n",
    "def vgg19_bn():\n",
    "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
    "    return VGG(make_layers(cfg['E'], batch_norm=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5,stride=1) # 28-5+1=24\n",
    "        self.conv2 = nn.Conv2d(6,16,5,stride=1) # 12-5+1=8\n",
    "        self.fc1 = nn.Linear(4*4*16,200)\n",
    "        self.fc2 = nn.Linear(200,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        if x.ndimension()==3:\n",
    "            x = x.unsqueeze(0)\n",
    "        o = F.relu(self.conv1(x))\n",
    "        o = F.avg_pool2d(o,2,2)\n",
    "\n",
    "        o = F.relu(self.conv2(o))\n",
    "        o = F.avg_pool2d(o,2,2)\n",
    "\n",
    "        o = o.view(o.shape[0],-1)\n",
    "        o = self.fc1(o)\n",
    "        o = F.relu(o)\n",
    "        o = self.fc2(o)\n",
    "        return o\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FNN,self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(784,500),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(500,500),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(500,500),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(500,10))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        o = self.net(x)\n",
    "        return o\n",
    "\n",
    "def lenet():\n",
    "    return LeNet()\n",
    "\n",
    "def fnn():\n",
    "    return FNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen_variance(net, criterion, dataloader, n_iters=10, tol=1e-2, verbose=False):\n",
    "    n_parameters = num_parameters(net)\n",
    "    v0 = torch.randn(n_parameters)\n",
    "\n",
    "    Av_func = lambda v: variance_vec_prod(net, criterion, dataloader, v)\n",
    "    mu = power_method(v0, Av_func, n_iters, tol, verbose)\n",
    "    return mu\n",
    "\n",
    "\n",
    "def eigen_hessian(net, criterion, dataloader, n_iters=10, tol=1e-2, verbose=False):\n",
    "    n_parameters = num_parameters(net)\n",
    "    v0 = torch.randn(n_parameters)\n",
    "\n",
    "    Av_func = lambda v: hessian_vec_prod(net, criterion, dataloader, v)\n",
    "    mu = power_method(v0, Av_func, n_iters, tol, verbose)\n",
    "    return mu\n",
    "\n",
    "\n",
    "def variance_vec_prod(net, criterion, dataloader, v):\n",
    "    X, y = dataloader.X, dataloader.y\n",
    "    Av, Hv, n_samples = 0, 0, len(y)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        bx, by = X[i:i+1].cuda(), y[i:i+1].cuda()\n",
    "        Hv_i = Hv_batch(net, criterion, bx, by, v)\n",
    "        Av_i = Hv_batch(net, criterion, bx, by, Hv_i)\n",
    "        Av += Av_i\n",
    "        Hv += Hv_i\n",
    "    Av /= n_samples\n",
    "    Hv /= n_samples\n",
    "    H2v = hessian_vec_prod(net, criterion, dataloader, Hv)\n",
    "    return Av - H2v\n",
    "\n",
    "\n",
    "def hessian_vec_prod(net, criterion, dataloader, v):\n",
    "    Hv_t = 0\n",
    "    n_batchs = len(dataloader)\n",
    "    dataloader.idx = 0\n",
    "    for _ in range(n_batchs):\n",
    "        bx, by = next(dataloader)\n",
    "        Hv_t += Hv_batch(net, criterion, bx.cuda(), by.cuda(), v)\n",
    "\n",
    "    return Hv_t/n_batchs\n",
    "\n",
    "\n",
    "def Hv_batch(net, criterion, batch_x, batch_y, v):\n",
    "    \"\"\"\n",
    "    Hessian vector multiplication\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    logits = net(batch_x)\n",
    "    loss = criterion(logits, batch_y)\n",
    "\n",
    "    grads = autograd.grad(loss, net.parameters(), create_graph=True, retain_graph=True)\n",
    "    idx, res = 0, 0\n",
    "    for grad_i in grads:\n",
    "        ng = torch.numel(grad_i)\n",
    "        v_i = v[idx:idx+ng].cuda()\n",
    "        res += torch.dot(v_i, grad_i.view(-1))\n",
    "        idx += ng\n",
    "\n",
    "    Hv = autograd.grad(res, net.parameters())\n",
    "    Hv = [t.data.cpu().view(-1) for t in Hv]\n",
    "    Hv = torch.cat(Hv)\n",
    "    return Hv\n",
    "\n",
    "\n",
    "def power_method(v0, Av_func, n_iters=10, tol=1e-3, verbose=False):\n",
    "    mu = 0\n",
    "    v = v0/v0.norm()\n",
    "    for i in range(n_iters):\n",
    "        time_start = time.time()\n",
    "\n",
    "        Av = Av_func(v)\n",
    "        mu_pre = mu\n",
    "        mu = torch.dot(Av,v).item()\n",
    "        v = Av/Av.norm()\n",
    "\n",
    "        if abs(mu-mu_pre)/abs(mu) < tol:\n",
    "            break\n",
    "        if verbose:\n",
    "            print('%d-th step takes %.0f seconds, \\t %.2e'%(i+1,time.time()-time_start,mu))\n",
    "    return mu\n",
    "\n",
    "\n",
    "def num_parameters(net):\n",
    "    \"\"\"\n",
    "    return the number of parameters for given model\n",
    "    \"\"\"\n",
    "    n_parameters = 0\n",
    "    for para in net.parameters():\n",
    "        n_parameters += para.data.numel()\n",
    "\n",
    "    return n_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Hyperparameters\n",
    "gpuid = '0,'\n",
    "dataset= 'fashionmnist'\n",
    "n_samples = 1000\n",
    "load_size = 1000\n",
    "optimizer = 'sgd'\n",
    "n_iters = 10000\n",
    "batch_size = 1000\n",
    "learning_rate = 1e-1\n",
    "momentum = 0.0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss().cuda()\n",
    "\n",
    "if dataset == 'fashionmnist':\n",
    "        train_loader, test_loader = load_fmnist(training_size=n_samples, batch_size=load_size)\n",
    "        net = fnn().cuda()\n",
    "        \n",
    "elif dataset == 'cifar10':\n",
    "        train_loader, test_loader = load_cifar10(training_size=n_samples, batch_size=load_size)\n",
    "        net = vgg11(num_classes=2).cuda()\n",
    "\n",
    "if optimizer == 'sgd':\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "elif optimizer == 'adam':\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "print(optimizer)\n",
    "\n",
    "print('===> Architecture:')\n",
    "print(net)\n",
    "\n",
    "print('===> Start training')\n",
    "train(net, criterion, optimizer, train_loader, batch_size, n_iters, verbose=True)\n",
    "\n",
    "train_loss, train_accuracy = eval_accuracy(net, criterion, train_loader)\n",
    "test_loss, test_accuracy = eval_accuracy(net, criterion, test_loader)\n",
    "print('===> Solution: ')\n",
    "print('\\t train loss: %.2e, acc: %.2f' % (train_loss, train_accuracy))\n",
    "print('\\t test loss: %.2e, acc: %.2f' % (test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('===> Compute sharpness:')\n",
    "sharpness = eigen_hessian(net, criterion, train_loader, n_iters=10, tol=1e-4, verbose=False)\n",
    "print('Sharpness is %.2e\\n'%(sharpness))\n",
    "\n",
    "print('===> Compute non-uniformity:')\n",
    "non_uniformity = math.sqrt(eigen_variance(net, criterion, train_loader, n_iters=10, tol=1e-4, verbose=True))\n",
    "print('Non-uniformity is %.2e\\n'%(non_uniformity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
